---
title: "MY472 - Assignment 3"
output: html_document
---

In this assignment, we will use Wikipedia as a training ground to study web scraping. There also exists a Wikidata API which can be more convenient for certain projects later, however, as the purpose of this assignment is to study web scraping, __do not use this or other APIs here__. Also please make sure to avoid sending too many requests to the website too quickly by adding some short pauses into the code.

When knitting your final markdown file, ensure that the code in Exercises 1-3 actually scrapes all information again. In other words, do not load data from files in interim steps where it is unclear how these files were created. Graders would need to see from the code that it scraped all information when it actually ran/was knitted.

```{r}
library("tidyverse")
library("rvest")
#install.packages("qdapRegex")
library("qdapRegex")
library("RColorBrewer")
#install.packages("rayshader")
library("rayshader")
#install.packages("remotes")
#remotes::install_github("tylermorganwall/rayshader")
library("rgl")
#install.packages("ggrepel")
library("ggrepel")
#install.packages("parzer")
library("parzer")
#install.packages("stringr", dependencies=TRUE)
library("stringr")
#install.packages("av")
library("av")
library("jpeg")
#install.packages("ggimage")
library("ggimage")
library("plotly")
library("scales")
```


## Exercise 1 (12 points)

Scrape the table with the 50 highest grossing films from https://en.wikipedia.org/wiki/List_of_highest-grossing_films, process the data, and store it in a data frame/tibble. Then use `ggplot2` to plot the data with the log rank on the x-axis, log world-wide gross on the y-axis, and points being annotated by movie titles. In the plot, do you see a pattern familiar from the lecture?

```{r}
# Your code here

url <- "https://en.wikipedia.org/wiki/List_of_highest-grossing_films"

# Storing url HTML
html_content <- read_html(url, stringsAsFactors=FALSE)

# Extracting tables 
tab <- html_table(html_content, fill = TRUE)

# Save as df (tibble)
top_50_data <- tab[[1]]
top_50_data <- as_tibble(top_50_data)
top_50_data

# Choosing relevant columns
top_50_data <- top_50_data[ , c(1,3,4)]

# Rename table columns
colnames(top_50_data) <- c('rank', 'title', 'gross')

# Remove commas and symbols
top_50_data$gross <-  gsub(",", "", top_50_data$gross)
top_50_data$gross <-  gsub("\\$", "", top_50_data$gross)
top_50_data$gross <-  gsub("F8", "", top_50_data$gross)
top_50_data$gross <-  gsub("F", "", top_50_data$gross)

# Convert to numbers
top_50_data$gross <-  as.numeric(top_50_data$gross)

# Plotting ggplot
p <- ggplot(top_50_data, aes(x = rank, y = gross, label=title)) + 
  geom_point(size=0.5, color="blue")

# Adding log10
p_log <- p + 
  scale_y_continuous(trans = 'log10', labels=comma) + 
  scale_x_continuous(trans = 'log10', labels=comma) 

# Adding label
p_log + 
  geom_label_repel(aes(label = title),
                  box.padding   = 0.35, 
                  point.padding = 0.5,
                  max.overlaps = 50,
                  size=1) +
  theme_classic()


```


------>
Pattern: It is similar to a power law distribution. The gross of a movie is inversely proportional to its rank. 

## Exercise 2 (28 points)

This exercise discusses three different approaches which are very helpful to know when scraping websites. The exemplary website we will look at is a simplified version of a recent LSE course list. You can find the website under the following link https://lse-my472.github.io/week05/data/lse_courses.html

In each of Parts 2.1-2.3, using `rvest`, the goal is to scrape all course titles and store them in a character vector with one element being one course title.

The first elements of such a vector should therefore be:

"AC411 Accounting, Strategy and Control",
"AC412 Accountability, Organisations and Risk Management",
"AC415 Management Accounting for Decision Making",
"AC416 Topics in Financial Reporting",
...

### 2.1 Approach one: Scraping all elements with one CSS selector

Inspect the website's source code in your browser to see which CSS selector would be able to select all course titles in one go. As a hint, have a think which HTML tag specifies the desired parts of the website and what its corresponding CSS selector is. Inserting this CSS selector into `html_elements()` will allow to scrape all course titles with one line of code. There is no need to copy the CSS selector with your browser or run a loop in this part of the exercise.

Print out the length of the final character vector storing all course titles (i.e. to show how many course titles were scraped) and also print out the first 20 elements/course titles. 

```{r}
# Your code here
url_2 <- 'https://lse-my472.github.io/week05/data/lse_courses.html'

# Reading url
courses_html <- read_html(url_2)

# Gathering all course titles
courses_nodes <- html_elements(courses_html, css = "p")

# Converting to text
courses_nodes_text <- html_text(courses_nodes)

# Print total length
length(courses_nodes_text)

# Print first 20 elements
courses_nodes_text[1:20]
```

### 2.2 Approach two: Loops to scrape element by element

When it is not easily possible to define a unifying CSS selector, another approach is copying element-specific CSS selectors with your browser, understanding their structure, and then building a loop (you can of course also copy the element-specific XPath instead of the CSS selector if you attempt the exercise after we have discussed XPaths in week 7). One usually copies and looks at selectors for a few elements until a pattern emerges that can be used in the loop. I have copied the CSS selectors obtained through Firefox of a few course titles in the first and second department and added them below.

First department (AC):

`body > div:nth-child(1) > table:nth-child(1) > tbody:nth-child(1) > tr:nth-child(2) > td:nth-child(1) > p:nth-child(1)`
`body > div:nth-child(1) > table:nth-child(1) > tbody:nth-child(1) > tr:nth-child(3) > td:nth-child(1) > p:nth-child(1)`
`body > div:nth-child(1) > table:nth-child(1) > tbody:nth-child(1) > tr:nth-child(4) > td:nth-child(1) > p:nth-child(1)`
`...`

Second department (AN):

`body > div:nth-child(1) > table:nth-child(2) > tbody:nth-child(1) > tr:nth-child(2) > td:nth-child(1) > p:nth-child(1)`
`body > div:nth-child(1) > table:nth-child(2) > tbody:nth-child(1) > tr:nth-child(3) > td:nth-child(1) > p:nth-child(1)`
`...`

Create a double for-loop to iterate over all departments and course titles. At each iteration of the loop, use the `sprintf` function to create the correct CSS selector or XPath as a string. Then use this string to scrape the respective course title and add it to a vector that stores all course titles.

At the end, print out the length of the final character vector storing all course titles (i.e. to show how many course titles were scraped) and also print out the first 20 elements/course titles. 

Hint 1: Have a look at the following code sample to see how `sprintf` can be used to change a string at each iteration of loops. The placeholders `%g` are replaced with the values of `a` and `b`.

```{r}

for (a in 1:3) {
  
  for (b in 1:3) {
    
    some_string <- sprintf("This string contains two changing integers %g and %g", a, b)
    
    print(some_string)
    
  }
  
}
```

Hint 2: One challenge is that each department has varying numbers of courses. One way is to choose a high number of iterations for the inner loop over course names in a department and to make sure the code keeps running even if respective CSS selectors or XPaths were not found for a given department-course observation, i.e. if that department offered fewer courses. In `rvest`, functions like `html_elements()` conveniently do not return an error if no element associated with a CSS selector or XPath was found but instead return a list of length zero and thereby allow the code to keep running.


```{r}
# Your code here

length_tables <- 24

crs1 = c()
crs6 = c()
for (c in 1:length_tables){
  crs_1 = courses_html %>%
    html_nodes(paste0('table:nth-child(',c,')'))
  e = (str_count(as.character(crs_1),"<tr>"))
  
  for (d in 2:e){
    crs2 = sprintf('body > div > table:nth-child(%g) > tbody > tr:nth-child(%g) > td > p',c,d)
    
    crs1[length(crs1)+1] = crs2
  }
}

for (j in crs1){
  
  crs7 = html_element(courses_html, css = j) %>% html_text()
  
  crs6[length(crs6)+1] = crs7
}

# Print total length
length(crs6)

# Print first 20 elements
print(crs6[1:20])

```


### 2.3 Approach three: Scrape content directly from the full HTLM text with regular expressions

A third approach can sometimes be to use no selectors at all, but rather define a regular expression which captures all relevant information directly from the page's full HTML text. The goal here is to define a regular expression which captures all course codes and to use this expression with the parsed HTML text in R.

Hint: You can just use `str_extract_all()` with the object `page_html_text` as input and a suitable regular expression to collect all course titles into a character vector (i.e. do not use any further `rvest` code here other than the line already added to the code chunk). Then print out the length of the final character vector storing all course titles (i.e. to show how many course titles were scraped) and also print out the first 20 elements/course titles. 

```{r}
page_html_text <- read_html("https://lse-my472.github.io/week05/data/lse_courses.html") %>% html_text()
# Your code here

crs_3 = as.character(courses_html)

str2 = str_extract_all(crs_3,regex("(?<=<p>).+(?=</p>)"),simplify = TRUE)

# Print length
length(str2)

# Print first 20 elements
print(str2[1:20])

```


## Exercise 3 (60 points)

This is an open-ended exercise that allows you to study web scraping using the English Wikipedia https://en.wikipedia.org/ while collecting and analysing information that you are particularly interested in. It could e.g. be about historical events, politics, science, arts, sports, economics, etc.

3.1 Write code which scrapes all information from Wikipedia that you would like to analyse and illustrate.

3.2 Clean all data and store it in one or more data frames/tibbles/quanteda objects etc.

3.3 Illustrate your findings. This could e.g. be a combination of computations with packages such as `dplyr`, text analysis with `quanteda`, visualisations with packages such as `ggplot2` or `plotly` or others, and/or written answers.

More extensive, carefully thought out, polished, and well understandable answers will receive more points. For general assessment criteria, also see the course website https://lse-my472.github.io/

Hint: Should you decide to use text analysis with `quanteda` as part of your project (note: there are many ways to answer this exercise and using `quanteda` is not necessary and only one option), a question might be how to load most of the text of Wikipedia articles into character vectors such that it can then be transformed into a `dfm` for applying dictionaries, analysing word shares, etc. One quick approximate approach is illustrated below. It combines all paragraphs of a respective Wikipedia page (this leaves out e.g. lists but often captures most of the textual information).

```{r}
sample_page <- "https://en.wikipedia.org/wiki/Claude_Shannon"

html_sample_page <- read_html(sample_page)

main_text_content <- html_sample_page %>% html_elements(css = "p") %>% html_text() %>% paste(collapse = " ")
```

As a side note, Shannon was one of the most important researchers of the 20th century and his ideas are the foundation of much of the information age, but he remains relatively unknown. If you have not heard of his research and are interested in learning more, e.g. have a look at this [trailer](https://youtu.be/E3OldEtfBrE) or his Wikipedia page.

Lastly, information on the Wikipedia website can be edited at any time. If it happens that your chosen article/s is/are subject to regular edits that could change the content/structure of the page (and possibly break your code) you may want to use the permanent link to a particular revision of your article. To get this link for an article, click ‘View history’ (top right), then the top entry for the time & date. This is not essential, but will ensure your code is reproducible when being graded.

----->

For this exercise, I am visualising the elevation difference of cities in Geneva. I am collecting data for each city from wikipedia at https://en.wikipedia.org/wiki/Municipalities_of_the_canton_of_Geneva
Data is scraped for each city page and stored in a data frame, to then visualise using ggplot and animate in 3D using rayshader. 

```{r}
# Your code here

url_3 <- "https://en.wikipedia.org/wiki/Municipalities_of_the_canton_of_Geneva"
canton_html <- read_html(url_3) # reading the HTML code

# gathering all the canton names
canton_names<- html_elements(canton_html, css = ".div-col li") %>% html_text()
canton_names

# converting into a table
canton_table <- tibble(name = canton_names, latitude = NA, longitude = NA, elevation = NA, postalcode = NA)
canton_table

# storing links
all_link_elements <- html_elements(canton_html, css = "a")
all_link_texts <- all_link_elements %>% html_text()

```

To clean texts from the website and store in the table, I am using the helper function from week 5 exercise 4 seminar by Friedrich Geiecke.

```{r}

# Helper function to clean texts (sourced from week05/exercise4 by Friedrich Geiecke)
helper_function <- function(x, labels, data) {
  
   x_plural <- paste0(x,"s")
   
   # Check singular
  if (x %in% labels) {
    text <- data[labels==x]
  }
  # Check plural
  else if (x_plural %in% labels) {
    text <- data[labels==x_plural]
    }
  else {return(NA)} 
  
  text <- text %>% str_replace_all("\n", "; ")
  text <- text %>% str_replace_all("^; ", "")
  text <- text %>% str_replace_all("\\[.*\\]", "")
  text <- text %>% str_replace_all("\\(.*\\)", "")
  text <- text %>% str_replace_all("([[:lower:]])([[:upper:]])", "\\1; \\2") 
 
  return(text)
  
}
```

A for loop is created to go to each municipality link from the geneva wikipedia page and get data from each link. The output data is stored in canton_table respectively. The output table is then cleaned further to be able to visualise it.

```{r}

# Loop over all canton names 
for (current_name in canton_names) {
  
  # Obtain relevant link element for the current city name
  current_canton_element <- all_link_elements[all_link_texts == current_name][1]
  
  # Navigate to URL of current city and Get target of hyperlink
  current_partial_url <- current_canton_element %>% html_attr("href")
  current_url <- paste("https://en.wikipedia.org",  current_partial_url, sep = "")
  # Load page
  current_html <- read_html(current_url)
  
  
  # Get labels and data
  labels <- current_html %>% html_elements(css = ".infobox-label") %>% html_text()
  data <- current_html %>% html_elements(css = ".infobox-data") %>% html_text()
  
  # Get coordinates
  lat <- current_html %>% html_elements(css = ".latitude") %>% html_text()
  long <- current_html %>% html_elements(css = ".longitude") %>% html_text()
  
  
  # Clean text and store in data frame
  canton_table[canton_table$name == current_name, "latitude"] <- lat[1]
  canton_table[canton_table$name == current_name, "longitude"] <- long[1]
  canton_table[canton_table$name == current_name, "elevation"] <- helper_function("Elevation", labels, data)
  canton_table[canton_table$name == current_name, "postalcode"] <- helper_function("Postal code(s)", labels, data)
  
  
  # Wait half a second before next request
  Sys.sleep(0.5)
  
}

canton_table

## Further cleaning table data

# Convert coordinates to decimal format
canton_table.dd = canton_table %>%
  mutate(lon = parzer::parse_lon(longitude),
         lat = parzer::parse_lat(latitude))

# Convert elevation column to numeric
canton_table.dd$elevation <-  as.character(canton_table.dd$elevation)
canton_table.dd$elevation <-  gsub("m", "", canton_table.dd$elevation)
canton_table.dd$elevation <- gsub('^\\s+|\\s+$', '', canton_table.dd$elevation)
canton_table.dd$elevation <- str_trim(canton_table.dd$elevation)
canton_table.dd$elevation[is.na(canton_table.dd$elevation)] <- "400"
canton_table.dd$elevation <-  as.numeric(canton_table.dd$elevation)

canton_table.dd

```

To visualise the data, I am using ggplot first and rayshader library to convert it in 3d to get a better visualisation of the elevation difference between cities in Geneva. The resulting plot is saved in the folder as movie_geneva.mp4 (as the knitted html file cannot display 3d plots).

*Inspired from Carlos Vecina, n.d https://typethepipe.com/post/ggplot-to-3d-in-r-with-rayshader/

```{r}

plot_cities <- ggplot(canton_table.dd,  aes(x=lon, y=lat)) + # base plot
  
  geom_point(aes(color=elevation), size=2) + # adding points
  
  scale_colour_gradient( limits=range(canton_table.dd$elevation), 
                        low="#FCB9B2", high="#B23A48") + # changing colour palette
    
  geom_text( # adding cities names
    aes(label=name),
    size=2,
    nudge_y = 0.01, 
    check_overlap = T
  ) + 
  
    labs(title = "Municipalities of the canton of Geneva",
       subtitle = "The 45 municipalities as of 2017 (From Wikipedia)", colour="Elevation in m") +
         
  theme(axis.line=element_blank(), 
        axis.text.x=element_blank(), axis.title.x=element_blank(),
        axis.text.y=element_blank(), axis.title.y=element_blank(),
        axis.ticks=element_blank(), 
        plot.title = element_text(hjust = 0.5),
        plot.subtitle = element_text(hjust = 0.5),
        panel.background = element_blank())   # clean everything 

plot_cities

plot_gg(plot_cities
            , width=4.0
            , height = 4.0
            , multicore = TRUE
            , windowsize = c(1400,866)
            , sunangle=225
            , zoom = 0.60
            , phi = 30
            , theta = 45
            , emboss_text = 0
            )

 # Close Windows
rgl.close()

# Saving file image
#render_snapshot(filename = "3D_geneva")

# Saving animated movie
#render_movie("movie_geneva.mp4",frames = 720, fps=30,zoom=0.6,fov = 30)
```
The animated 3D visualisation is saved in the folder as movie_geneva.mp4 as the knitted html document couldn't load it. (FILE saved in folder)